{ 
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.src_attn = MultiHeadedAttention(1,hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output,attn_weights = self.src_attn(output, output, output)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.self_attn = MultiHeadedAttention(1,hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = self.dropout(embedded)\n",
    "        output,attn_weights = self.self_attn(output,output,output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        # output and hidden are the same vectors\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    mask=None\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        mask=None\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x), self.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def prepareReference(lang, sentence):\n",
    "    # what this does is basicallyp prepares thee refernece and removes the <UNK> data. \n",
    "    # lang1 - str \n",
    "    # lang2 - str \n",
    "    words = sentence.split(\" \")\n",
    "    current = []\n",
    "    for word in words:\n",
    "        if lang.word2index.get(word) is not None:\n",
    "            current.append(word)\n",
    "        else:\n",
    "            current.append(\"UNK\")\n",
    "    return \" \".join(current)\n",
    "\n",
    "def readLangs(input_file, target_file, input_lang, target_lang, size):\n",
    "    # this handles for Chinese\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    input_lines = open(input_file, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "    target_lines = open(target_file, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "    if input_lang == \"zh\":\n",
    "        is_not_thing = lambda x: x is not ''\n",
    "        cleaned_list = list(filter(is_not_thing, input_lines))\n",
    "        input_lines = [' '.join(s.split()) for s in cleaned_list]\n",
    "        pdb.set_trace()\n",
    "        target_pair = [normalizeString(s) for s in target_lines]\n",
    "        pairs = list(zip(input_lines, target_lines))\n",
    "    else:\n",
    "        lines = list(zip(input_lines, target_lines))\n",
    "        # Split every line into pairs and normalize\n",
    "        pairs = [[normalizeString(s) for s in l] for l in lines]\n",
    "    print(pairs[0])\n",
    "\n",
    "    input_lang = Lang(input_lang)\n",
    "    target_lang = Lang(target_lang)\n",
    "\n",
    "    return input_lang, target_lang, pairs\n",
    "\n",
    "def preparTraineData(input_file, target_file, input_lang, target_lang, size):\n",
    "    input_lang, target_lang, pairs = readLangs(input_file, target_file, input_lang, target_lang, size)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    print(pairs[0])\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        target_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(target_lang.name, target_lang.n_words)\n",
    "    return input_lang, target_lang, pairs\n",
    "\n",
    "def prepareDataInitial(lang1, lang2):\n",
    "    # This sts up everything you need for preprocessing. \n",
    "    input_file = 'iwslt-zh-en/train.tok.zh'\n",
    "    target_file = 'iwslt-zh-en/train.tok.en'\n",
    "    input_lang_train, target_lang_train, pairs = prepareTrainData(input_file, target_file, 'zh', 'eng', size=50000)\n",
    "    pickle.dump(pairs, open(\"preprocessed_data_no_elmo/iwslt-zh-eng/preprocessed_no_indices_pairs_train\", \"wb\"))\n",
    "    pickle.dump(input_lang_train, open(\"preprocessed_data_no_elmo/iwslt-\"+lang1+\"-\"+lang2+\"/preprocessed_no_elmo_\"+lang1+\"lang\", \"wb\"))\n",
    "    pickle.dump(target_lang_train, open(\"preprocessed_data_no_elmo/iwslt-\"+lang1+\"-\"+lang2+\"/preprocessed_no_elmo_\"+lang2+\"lang\", \"wb\"))\n",
    "    lang2 = \"eng\"\n",
    "    for lang1 in [\"zh\", \"vi\"]:\n",
    "        for dataset in [\"validation\", \"test\"]:\n",
    "            input_lang = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-\"+lang1+\"-\"+lang2+\"/preprocessed_no_elmo_\"+lang1+\"lang\")\n",
    "            target_lang = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-\"+lang1+\"-\"+lang2+\"/preprocessed_no_elmo_englang\")\n",
    "            if dataset == \"validation\":\n",
    "                source_language =  open(\"iwslt-\"+lang1+\"-en/dev.tok.\"+lang1, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "                actual_english_test = open(\"iwslt-\"+lang1+\"-en/dev.tok.en\", encoding='utf-8').read().strip().split(\"\\n\")\n",
    "            else:\n",
    "                source_language = open(\"iwslt-\"+lang1+\"-en/\"+dataset+\".tok.\"+lang1, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "                actual_english_test = open(\"iwslt-\"+lang1+\"-en/\"+dataset+\".tok.en\", encoding='utf-8').read().strip().split(\"\\n\")\n",
    "            if lang1 == \"vi\":\n",
    "                tensors_input = [tensorFromSentence(input_lang, normalizeString(s), 0) for s in source_language]\n",
    "            elif lang1 == \"zh\":\n",
    "                # don't normalize\n",
    "                tensors_input = [tensorFromSentence(input_lang,s, 0) for s in source_language]\n",
    "            reference_convert =[prepareReference(target_lang, normalizeString(s)) for s in actual_english_test]\n",
    "            final_pairs = list(zip(tensors_input, reference_convert))\n",
    "            pdb.set_trace()\n",
    "            pickle.dump(final_pairs, open(\"preprocessed_data_no_elmo/iwslt-\"+lang1+\"-\"+lang2+\"/preprocessed_no_indices_pairs_\"+ dataset+\"_tokenized\", \"wb\"))\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def indexesFromSentence(lang, sentence, i):\n",
    "    try:\n",
    "        words = sentence.split(' ')\n",
    "    except Exception:\n",
    "        pdb.set_trace()\n",
    "    indices = []\n",
    "    for word in words:\n",
    "        if lang.word2index.get(word) is not None:\n",
    "            indices.append(lang.word2index[word])\n",
    "        else:\n",
    "            indices.append(UNK_token) # UNK_INDEX\n",
    "    return indices\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence, i):\n",
    "    indexes = indexesFromSentence(lang, sentence, i)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair, input_lang, target_lang, i):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0], i)\n",
    "    target_tensor = tensorFromSentence(target_lang, pair[1], i)\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        # output and hidden are the same vectors\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class EncoderRNNBidirectional(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNNBidirectional, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        # output and hidden are the same vectors\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNNBidirectional(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNNBidirectional, self).__init__()\n",
    "        self.hidden_size = hidden_size*2\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size*2)\n",
    "        self.gru = nn.GRU(hidden_size*2,  hidden_size*2)\n",
    "        self.out = nn.Linear(hidden_size*2, hidden_size*4) # sincec output_size >> hidden_size, we increase \n",
    "        # this\n",
    "        self.out2 = nn.Linear(hidden_size*4, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden.view(1, 1, -1))\n",
    "        output = self.out(output[0])\n",
    "        output = F.relu(output)\n",
    "        output = self.softmax(self.out2(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "# example of input_tensor: [2, 43, 23, 9, 19, 4]. Indexed on our vocabulary. \n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, decoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # iterate GRU over words --> final hidden state is representation of source sentence. \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0,0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "\n",
    "def load_cpickle_gc(dirlink):\n",
    "    # https://stackoverflow.com/questions/26860051/how-to-reduce-the-time-taken-to-load-a-pickle-file-in-python\n",
    "    output = open(dirlink, 'rb')\n",
    "\n",
    "    # disable garbage collector\n",
    "    gc.disable()\n",
    "\n",
    "    mydict = pickle.load(output)\n",
    "\n",
    "    # enable garbage collector again\n",
    "    gc.enable()\n",
    "    output.close()\n",
    "    return mydict\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters,n_epochs,  lang1, lang2, max_length, max_length_generation, print_every=5000, plot_every=5000, learning_rate=3e-4, search=\"beam\"):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang o|bject for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    n_iters is the number of training pairs per epoch you want to train on\n",
    "    \"\"\"\n",
    "    training_pairs = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-\"+lang1.name+\"-\"+lang2.name+\"/preprocessed_no_indices_pairs_train_tokenized\")\n",
    "    n_iters = len(training_pairs)\n",
    "    validation_pairs = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-\"+lang1.name+\"-\"+lang2.name+\"/preprocessed_no_indices_pairs_validation_tokenized\")\n",
    "    pickle.dump(validation_pairs, open(\"preprocessed_data_no_elmo/iwslt-zh-eng/preprocessed_no_indices_pairs_validation_tokenized\", \"wb\"))\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0\n",
    "    val_loss_total = 0\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max')\n",
    "    #training_pairs = [tensorsFromPair(pair, lang1, lang2, 0) for pair in pairs]\n",
    "    for i in range(n_epochs):\n",
    "        criterion = nn.NLLLoss()\n",
    "        # framing it as a categorical loss function. \n",
    "        for iter in range(1, n_iters + 1):\n",
    "            if iter % 100 == 0:\n",
    "                print(iter)\n",
    "            training_pair = training_pairs[iter - 1] \n",
    "            d_input_tensor = training_pair[0]\n",
    "            d_target_tensor = training_pair[1]\n",
    "            loss = train(d_input_tensor, d_target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, iter / n_epochs),\n",
    "                                             iter, iter / n_epochs * 100, print_loss_avg))\n",
    "                val_loss = test_model(encoder, decoder,search, validation_pairs, lang1,max_length,  max_length_generation)\n",
    "                #scheduler.step(val_loss)\n",
    "                # retursn teh bleu score\n",
    "                print(\"VALIDATION BLEU SCORE: \"+str(val_loss))\n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                val_loss_avg = val_loss_total/plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                val_losses.append(val_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                val_loss_total = 0\n",
    "                torch.save(encoder.state_dict(), \"encoder_\"+str(i)+str(iter)+str(lang1.name)+str(lang2.name))\n",
    "                torch.save(decoder.state_dict(), \"decoder_\"+str(i)+str(iter)+str(lang1.name)+str(lang2.name))\n",
    "                pickle.dump(plot_loss_avg, open(\"training_loss\"+str(lang1.name)+str(lang2.name), \"wb\"))\n",
    "                pickle.dump(val_loss_avg, open(\"val_loss\"+str(lang1.name)+str(lang2.name), \"wb\"))\n",
    "    \n",
    "    showPlot(plot_losses)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def greedy_search(decoder, decoder_input, hidden, max_length):\n",
    "    translation = []\n",
    "    for i in range(max_length):\n",
    "        next_word_softmax, hidden = decoder(decoder_input, hidden)\n",
    "        best_idx = torch.max(next_word_softmax, 1)[1].squeeze().item()\n",
    "\n",
    "        # convert idx to word\n",
    "        best_word = target_lang.index2word[best_idx]\n",
    "        translation.append(best_word)\n",
    "        decoder_input = torch.tensor([[best_idx]], device=device)\n",
    "        \n",
    "        if best_word == 'EOS':\n",
    "            break\n",
    "    return translation\n",
    "\n",
    "\n",
    "def beam_search(decoder, decoder_input, hidden, max_length, k = 2):\n",
    "    \n",
    "    candidates = [(decoder_input, 0, hidden)]\n",
    "    potential_candidates = []\n",
    "    completed_translations = []\n",
    "\n",
    "    # put a cap on the length of generated sentences\n",
    "    for m in range(max_length):\n",
    "        for c in candidates:\n",
    "            # unpack the tuple\n",
    "            c_sequence = c[0]\n",
    "            c_score = c[1]\n",
    "            c_hidden = c[2]\n",
    "            # EOS token\n",
    "            if c_sequence[-1] == 1:\n",
    "                completed_translations.append((c_sequence, c_score))\n",
    "                k = k - 1\n",
    "            else:\n",
    "                next_word_probs, hidden = decoder(c_sequence[-1], c_hidden)\n",
    "                # in the worst-case, one sequence will have the highest k probabilities\n",
    "                # so to save computation, only grab the k highest_probability from each candidate sequence\n",
    "                top_probs, top_idx = torch.topk(next_word_probs, k)\n",
    "                for i in range(len(top_probs[0])):\n",
    "                    word = torch.from_numpy(np.array([top_idx[0][i]]).reshape(1, 1)).to(device)\n",
    "                    new_score = c_score + top_probs[0][i]\n",
    "                    potential_candidates.append((torch.cat((c_sequence, word)).to(device), new_score, c_hidden))\n",
    "\n",
    "        candidates = sorted(potential_candidates, key= lambda x: x[1])[0:k] \n",
    "        potential_candidates = []\n",
    "\n",
    "    completed = completed_translations + candidates\n",
    "    completed = sorted(completed, key= lambda x: x[1])[0] \n",
    "    final_translation = []\n",
    "    for x in completed[0]:\n",
    "        final_translation.append(target_lang.index2word[x.squeeze().item()])\n",
    "    return final_translation\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10, strategy=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Randomly select a English sentence from the dataset and try to produce its French translation.\n",
    "    Note that you need a correct implementation of evaluate() in order to make this function work.\n",
    "    \"\"\"    \n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = generate_translation(encoder, decoder, pair[0], search=strategy)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "def evaluate(encoder, decoder, sentence,max_length,  max_length_generation, search=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Function that generate translation.\n",
    "    First, feed the source sentence into the encoder and obtain the hidden states from encoder.\n",
    "    Secondly, feed the hidden states into the decoder and unfold the outputs from the decoder.\n",
    "    Lastly, for each outputs from the decoder, collect the corresponding words in the target language's vocabulary.\n",
    "    And collect the attention for each output words.\n",
    "    @param encoder: the encoder network\n",
    "    @param decoder: the decoder network\n",
    "    @param sentence: string, a sentence in source language to be translated\n",
    "    @param max_length: the max # of words that the decoder can return\n",
    "    @output decoded_words: a list of words in target language\n",
    "    @output decoder_attentions: a list of vector, each of which sums up to 1.0\n",
    "    \"\"\"    \n",
    "    # process input sentence\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence # this is already tokenized to a pair so it doens't \n",
    "        # take as long to run. \n",
    "        input_length = input_tensor.size()[0]\n",
    "        # encode the source lanugage\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        # decode the context vector\n",
    "        decoder_hidden = encoder_hidden # decoder starts from the last encoding sentence\n",
    "        # output of this function\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "        \n",
    "        if search == 'greedy':\n",
    "            decoded_words = greedy_search(decoder, decoder_input, decoder_hidden, max_length_generation)\n",
    "        elif search == 'beam':\n",
    "            decoded_words = beam_search(decoder, decoder_input, decoder_hidden, max_length_generation)  \n",
    "        return decoded_words\n",
    "\n",
    "\n",
    "import sacrebleu\n",
    "def calculate_bleu(predictions, labels):\n",
    "\t\"\"\"\n",
    "\tOnly pass a list of strings \n",
    "\t\"\"\"\n",
    "\t# tthis is ony with n_gram = 4\n",
    "\n",
    "\tbleu = sacrebleu.raw_corpus_bleu(predictions, [labels], .01).score\n",
    "\treturn bleu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(encoder, decoder,search, test_pairs, lang1,max_length, max_length_generation):\n",
    "    # for test, you only need the lang1 words to be tokenized,\n",
    "    # lang2 words is the true labels\n",
    "    encoder_inputs = [pair[0] for pair in test_pairs]\n",
    "    true_labels = [pair[1] for pair in test_pairs]\n",
    "    translated_predictions = []\n",
    "    for i in range(len(encoder_inputs)): \n",
    "        if i% 100== 0:\n",
    "            print(i)\n",
    "        e_input = encoder_inputs[i]\n",
    "        decoded_words = evaluate(encoder, decoder, e_input, max_length, max_length_generation)\n",
    "        translated_predictions.append(\" \".join(decoded_words))\n",
    "    print(translated_predictions[0])\n",
    "    print(true_labels[0])\n",
    "    return calculate_bleu(translated_predictions, true_labels)\n",
    "\n",
    "input_lang = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_elmo_vilang\")\n",
    "target_lang = load_cpickle_gc(\"preprocessed_data_no_elmo/iwslt-vi-eng/preprocessed_no_elmo_englang\")\n",
    "hidden_size = 256\n",
    "self_attention = True\n",
    "attention = False\n",
    "\n",
    "if self_attention == True:\n",
    "    encoder = SelfAttnEecoderRNN(input_lang.n_words, hidden_size, n_layers=1, dropout_p=0.1).to(device))\n",
    "else:\n",
    "    encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "\n",
    "if attention == True:\n",
    "    decoder = AttnDecoderRNN(target_lang.n_words, hidden_size, n_layers=1, dropout_p=0.1).to(device)\n",
    "else:\n",
    "    decoder = DecoderRNN(target_lang.n_words, hidden_size).to(device)\n",
    "    \n",
    "encoder.load_state_dict(torch.load(\"encoder_1055000vieng\"))\n",
    "decoder.load_state_dict(torch.load(\"decoder_1055000vieng\"))\n",
    "total_zh_en_train_pairs_length = 13376 \n",
    "n_iters = 10\n",
    "n_epochs = 10\n",
    "max_length_chinese = 530 # for chinese\n",
    "max_length_viet = 759\n",
    "max_generation = 619 # the maximum number of generation for vietnamese is the maxength of english trnaslation \n",
    "# this is the same for both \n",
    "trainIters(encoder, decoder, n_iters,n_epochs, input_lang, target_lang, max_length_viet, max_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    \"\"\"\n",
    "    Function that takes in attention and visualize the attention.\n",
    "    @param - input_sentence: string the represent a list of words from source language\n",
    "    @param - output_words: the gold translation in target language\n",
    "    @param - attentions: a numpy array\n",
    "    \"\"\"\n",
    "    # Set up figure with colorbar    \n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # TODO: Add your code here to visualize the attention\n",
    "    # look at documentation for imshow https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.matshow.html\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, decoder, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "# input a sentence\n",
    "evaluateAndShowAttention(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
