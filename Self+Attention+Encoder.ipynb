{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import math, copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "from model_architectures import Encoder_RNN, Decoder_RNN\n",
    "from data_prep import prepareData, tensorsFromPair, prepareNonTrainDataForLanguagePair, load_cpickle_gc\n",
    "from inference import generate_translation\n",
    "from misc import timeSince, load_cpickle_gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "PAD_token = 0\n",
    "PAD_TOKEN = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "teacher_forcing_ratio = 1.0\n",
    "attn_model = 'dot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_lang = load_cpickle_gc(\"input_lang_vi\")\n",
    "target_lang = load_cpickle_gc(\"target_lang_en\")\n",
    "train_idx_pairs = load_cpickle_gc(\"train_vi_en_idx_pairs\")\n",
    "val_idx_pairs = load_cpickle_gc(\"val_idx_pairs\")\n",
    "val_pairs = load_cpickle_gc(\"val_pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguagePairDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sent_pairs): \n",
    "        # this is a list of sentences \n",
    "        self.sent_pairs_list = sent_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent_pairs_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        sent1 = self.sent_pairs_list[key][0]\n",
    "        sent2 = self.sent_pairs_list[key][1]\n",
    "        return [sent1, sent2, len(sent1), len(sent2)]\n",
    "\n",
    "def language_pair_dataset_collate_function(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent1_length_list = []\n",
    "    sent2_list = []\n",
    "    sent2_length_list = []\n",
    "    # padding\n",
    "    # NOW PAD WITH THE MAXIMUM LENGTH OF THE FIRST and second batches \n",
    "    max_length_1 = max([len(x[0]) for x in batch])\n",
    "    max_length_2 = max([len(x[1]) for x in batch])\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]).T.squeeze(), pad_width=((0,max_length_1-len(datum[0]))), \n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        padded_vec_2 = np.pad(np.array(datum[1]).T.squeeze(), pad_width=((0,max_length_2-len(datum[1]))), \n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        sent1_list.append(padded_vec_1)\n",
    "        sent2_list.append(padded_vec_2)\n",
    "        sent1_length_list.append(len(datum[0]))\n",
    "        sent2_length_list.append(len(datum[1]))\n",
    "    return [torch.from_numpy(np.array(sent1_list)), torch.cuda.LongTensor(sent1_length_list), \n",
    "            torch.from_numpy(np.array(sent2_list)), torch.cuda.LongTensor(sent2_length_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = LanguagePairDataset(train_idx_pairs)\n",
    "# is there anything in the train_idx_pairs that is only 0s right noww instea dof padding. \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=language_pair_dataset_collate_function,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Check SOS Token: Do both train and val start with that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SupEncoder(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A super class of Encoder that learns embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, src_embed):\n",
    "        super(SupEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.src_embed = src_embed \n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"Take in and process masked src sequences.\"\n",
    "        print('encoder')\n",
    "        return self.encode(src, src_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    \n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.linears[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the weights here\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "       \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        # hidden is 32 by 256\n",
    "        # encoder_outputs is 32 by 72 by 256\n",
    "#         batch_size = hidden.size()[0]\n",
    "#         if self.self_attn == False:\n",
    "#             hidden = hidden[0]\n",
    "        hidden = hidden[0]\n",
    "        batch_size = hidden.size()[0]\n",
    "        attn_energies = []\n",
    "        for i in range(batch_size):\n",
    "            attn_energies.append(self.score(hidden[i], encoder_outputs[i]))\n",
    "        \n",
    "        # attn_energies is 32 by 72\n",
    "        attn_energies = self.softmax(torch.stack(attn_energies))\n",
    "        \n",
    "        context_vectors = []\n",
    "        for i in range(batch_size):\n",
    "            context_vectors.append(torch.matmul(attn_energies[i], encoder_outputs[i]))\n",
    "                \n",
    "        context_vectors = torch.stack(context_vectors)\n",
    "        \n",
    "        return context_vectors\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':            \n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 22 by 256\n",
    "            encoder_output = torch.transpose(encoder_output, 0, 1)\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, self.attn(encoder_output))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            len_encoder_output = encoder_output.size()[1]\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            hidden = torch.transpose(hidden, 0, 1)\n",
    "            # hidden is 256 by 1\n",
    "            hidden = hidden.repeat(hidden_size, len_encoder_output)\n",
    "            # hidden is 256 by 22\n",
    "            concat = torch.cat((hidden, encoder_output), dim=0)\n",
    "            # concat is 512 by 22\n",
    "            # self.attn(concat) --> 256 by 22\n",
    "            energy = torch.matmul(self.v, torch.tanh(self.attn(concat)))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class SelfAttnEncoderRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "#         super(SelfAttnEncoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.dropout = dropout\n",
    "            \n",
    "#         # Define layers\n",
    "#         self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.embedding_dropout = nn.Dropout(dropout)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True,dropout=dropout)\n",
    "#         self.out = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "#         # Choose attention model\n",
    "#         if attn_model != 'none':\n",
    "#             self.attn = SelfAttn(attn_model, hidden_size, True)\n",
    "\n",
    "        \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "#     def forward(self, sents, sent_lengths):\n",
    "#         '''\n",
    "#             sents is (batch_size by padded_length)\n",
    "#             when we evaluate sentence by sentence, you evaluate it with batch_size = 1, padded_length.\n",
    "#             [[1, 2, 3, 4]] etc. \n",
    "#         '''\n",
    "#         batch_size = sents.size()[0]\n",
    "#         sent_lengths = list(sent_lengths)\n",
    "#         # We sort and then do pad packed sequence here. \n",
    "#         descending_lengths = [x for x, _ in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "#         descending_indices = [x for _, x in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "#         descending_lengths = np.array(descending_lengths)\n",
    "#         descending_sents = torch.index_select(sents, 0, torch.tensor(descending_indices).to(device))\n",
    "\n",
    "#         # get embedding\n",
    "#         embed = self.embedding(descending_sents)\n",
    "#         embed = self.embedding_dropout(embed)\n",
    "#         # pack padded sequence\n",
    "#         embed = torch.nn.utils.rnn.pack_padded_sequence(embed, descending_lengths, batch_first=True)\n",
    "\n",
    "# #         embed = embed.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "        \n",
    "#         # fprop though RNN\n",
    "#         self.hidden = self.init_hidden(batch_size)       \n",
    "#         rnn_out, self.hidden = self.gru(embed, self.hidden)\n",
    "#         rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "#         # rnn_out is 32 by 72 by 256\n",
    "       \n",
    "#         # change the order back\n",
    "#         change_it_back = [x for _, x in sorted(zip(descending_indices, range(len(descending_indices))))]\n",
    "#         self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device))  \n",
    "#         rnn_out = torch.index_select(rnn_out, 0, torch.LongTensor(change_it_back).to(device)) \n",
    "        \n",
    "#         # apply to encoder outputs to get weighted average\n",
    "#         rnn_out = self.attn(rnn_out, rnn_out) \n",
    "        \n",
    "# #         torch.Size([32, 72, 256]) torch.Size([1, 32, 256])\n",
    "#         return rnn_out, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "            \n",
    "    def init_hidden(self, batch_size):\n",
    "#         return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "         return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size()[0]\n",
    "        # Note: we run this one step at a time\n",
    "        last_hidden = self.init_hidden(batch_size)   \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        input_seq = input_seq\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "      \n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context = self.attn(rnn_output, encoder_outputs)\n",
    "        # context is 32 by 256\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        # rnn_output is 32 by 256\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "        # output is 32 by vocab_size\n",
    "        output = self.LogSoftmax(output)\n",
    "\n",
    "        # Return final output, hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, pairs, validation_pairs, lang1, lang2, search, title, max_length_generation,  print_every=1000, plot_every=1000, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang object for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    Max length generation is the max length generation you want \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    plot_losses, val_losses = [], []\n",
    "    val_losses = [] \n",
    "    count, print_loss_total, plot_loss_total, val_loss_total, plot_val_loss = 0, 0, 0, 0, 0 \n",
    "    encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate)\n",
    "    #encoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode=\"min\")\n",
    "    #decoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode=\"min\")\n",
    "\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token) # this ignores the padded token. \n",
    "    plot_loss =[]\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        plot_loss = []\n",
    "        val_loss = []\n",
    "        for step, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(train_loader):\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "#             sent1_batch, sent2_batch = sent1s.to(device), sent2s.to(device) \n",
    "            sent1_batch, sent2_batch = sent1s, sent2s\n",
    "#             sent1_length_batch, sent2_length_batch = sent1_lengths.to(device), sent2_lengths.to(device)\n",
    "            sent1_length_batch, sent2_length_batch = sent1_lengths, sent2_lengths\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            encoder_outputs = encoder(sent1_batch, None)\n",
    "            # outputs is 32 by 72 by 256\n",
    "            # encoder_hidden is 1 by 32 by 256\n",
    "            \n",
    "#             decoder_input = torch.LongTensor([SOS_token] * BATCH_SIZE).view(-1, 1).to(device)\n",
    "            decoder_input = torch.LongTensor([SOS_token] * BATCH_SIZE).view(-1, 1)\n",
    "            decoder_hidden = encoder_outputs\n",
    "            # decoder_input is 32 by 1\n",
    "            # decoder_hidden is 1 by 32 by 256\n",
    "                        \n",
    "            max_trg_len = max(sent2_lengths)\n",
    "            loss = 0\n",
    "            \n",
    "            # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "            for t in range(max_trg_len):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "                # decoder_output is 32 by vocab_size\n",
    "                # sent2_batch is 32 by 46\n",
    "                loss += criterion(decoder_output, sent2_batch[:, t])\n",
    "                decoder_input = sent2_batch[:, t]\n",
    "             \n",
    "            loss = loss / max_trg_len.float().cpu()\n",
    "            print_loss_total += loss\n",
    "            count += 1\n",
    "            print(\"train loss\", loss)\n",
    "            loss.backward()\n",
    "        \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            if (step+1) % print_every == 0:\n",
    "                # lets train and plot at the same time. \n",
    "                print_loss_avg = print_loss_total / count\n",
    "                count = 0\n",
    "                print_loss_total = 0\n",
    "                print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, step / n_epochs),\n",
    "                                             step, step / n_epochs * 100, print_loss_avg))\n",
    "                # 42s\n",
    "#                 v_loss = test_model(encoder, decoder, search, validation_pairs, lang2, max_length=max_length_generation)\n",
    "                # returns bleu score\n",
    "#                 print(\"VALIDATION BLEU SCORE: \"+str(v_loss))\n",
    "#                 val_loss.append(v_loss)\n",
    "                plot_loss.append(print_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    save_model(encoder, decoder, val_losses, plot_losses, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "# number of duplicate layers in encoder\n",
    "N = 6\n",
    "# number of heads\n",
    "h=8\n",
    "dropout=0.1\n",
    "\"Helper: Construct a model from hyperparameters.\"\n",
    "# c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(h, hidden_size) \n",
    "ff = PositionwiseFeedForward(hidden_size,input_lang.n_words, dropout) \n",
    "position = PositionalEncoding(hidden_size, dropout) \n",
    "# src_embed = nn.Sequential(Embeddings(hidden_size, input_lang.n_words), c(position)) \n",
    "src_embed = nn.Sequential(Embeddings(hidden_size, input_lang.n_words), position) \n",
    "# encoder1 = SupEncoder(Encoder(EncoderLayer(hidden_size, c(attn), c(ff), dropout), N),src_embed) \n",
    "encoder1 = SupEncoder(Encoder(EncoderLayer(hidden_size, attn, ff, dropout), N),src_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zl2516/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "train loss tensor(10.6571, grad_fn=<DivBackward1>)\n",
      "train loss tensor(10.5974, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# encoder1 = SelfAttnEncoderRNN(input_lang.n_words, hidden_size,1).to(device)\n",
    "decoder1 = LuongAttnDecoderRNN(attn_model, hidden_size, target_lang.n_words, 1)\n",
    "args = {\n",
    "    'n_epochs': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'search': 'beam',\n",
    "    'encoder': encoder1,\n",
    "    'decoder': decoder1,\n",
    "    'lang1': input_lang, \n",
    "    'lang2': target_lang,\n",
    "    \"pairs\":train_idx_pairs, \n",
    "    \"validation_pairs\": val_idx_pairs[:200], \n",
    "    \"title\": \"Training Curve for Basic 1-Directional Encoder Decoder Model With LR = 0.0001\",\n",
    "    \"max_length_generation\": 20, \n",
    "    \"plot_every\": 10, \n",
    "    \"print_every\": 10\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "We follow https://arxiv.org/pdf/1406.1078.pdf \n",
    "and use the Adadelta optimizer\n",
    "\n",
    "\"\"\"\n",
    "print(BATCH_SIZE)\n",
    "\n",
    "trainIters(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to add to(device) for tensors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
