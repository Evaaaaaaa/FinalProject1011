{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, copy, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext import data, datasets\n",
    "from misc import timeSince, load_cpickle_gc\n",
    "import seaborn\n",
    "\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# if device == 'cuda':\n",
    "#     Tensor = torch.CudaTensor\n",
    "# else:\n",
    "#     Tensor = torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "#         return x + self.dropout(sublayer(self.norm(x).cuda()).cuda()).cuda()\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        \n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         return self.w_2(self.dropout(F.relu(self.w_1(x).cuda()).cuda()).cuda()).cuda()\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "#  do i have to use cuda here for Encoder, Encoderlayer, Decoder, Decoderlayer, Embeddings\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "#     model = EncoderDecoder(\n",
    "#         Encoder(EncoderLayer(d_model, attn, ff, dropout), N),\n",
    "#         Decoder(DecoderLayer(d_model, attn, attn, \n",
    "#                              ff, dropout), N),\n",
    "#         nn.Sequential(Embeddings(d_model, src_vocab), position),\n",
    "#         nn.Sequential(Embeddings(d_model, tgt_vocab), position),\n",
    "#         Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "#     model.cuda()\n",
    "    for i, batch in enumerate(data_iter):\n",
    "#         out = model.forward(batch.src.cuda(), batch.trg.cuda(), \n",
    "#                             batch.src_mask, batch.trg_mask)\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "#         if mask.dim() > 0:\n",
    "# #             int dim, Tensor index, Tensor value\n",
    "#             true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x.float(), Variable(true_dist.float(), requires_grad=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zl2516/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zl2516/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0fe9feeb00>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fea421780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "#         a = x.contiguous().view(-1, x.size(-1)).type(torch.FloatTensor)\n",
    "#         b = y.contiguous().view(-1).type(torch.FloatTensor)\n",
    "#         norm = norm.type(torch.FloatTensor)\n",
    "#         loss = self.criterion(a,b) / norm\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm.float()\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data[0] * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return Batch(src, trg, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    def tokenize_vi(text):\n",
    "        return [tok.text for tok in input_lang.tokenizer(text)]\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in output_lang.tokenizer(text)]\n",
    "\n",
    "    BOS_WORD = '<s>'\n",
    "    EOS_WORD = '</s>'\n",
    "    BLANK_WORD = \"<blank>\"\n",
    "#     SRC = data.Field(tokenize=tokenize_vi, pad_token=BLANK_WORD)\n",
    "    SRC = data.Field(pad_token=BLANK_WORD)\n",
    "\n",
    "#     TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "#                      eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "    TGT = data.Field(init_token = BOS_WORD,eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "    \n",
    "    MAX_LEN = 100\n",
    "\n",
    "    train = datasets.TranslationDataset(path='iwslt-vi-en-processed/train.tok',\n",
    "        exts=('.vi', '.en'), fields=(SRC, TGT), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    val = datasets.TranslationDataset(path='iwslt-vi-en-processed/dev.tok',\n",
    "        exts=('.vi', '.en'), fields=(SRC, TGT), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    test = datasets.TranslationDataset(path='iwslt-vi-en-processed/test.tok',\n",
    "        exts=('.vi', '.en'), fields=(SRC, TGT), \n",
    "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)\n",
    "    \n",
    "    MIN_FREQ = 2\n",
    "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zl2516/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/home/zl2516/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "#     model.cuda()\n",
    "    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "    BATCH_SIZE = 32\n",
    "    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=device,\n",
    "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                            batch_size_fn=batch_size_fn, train=True)\n",
    "    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=device,\n",
    "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                            batch_size_fn=batch_size_fn, train=False)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
    "                  model, \n",
    "                  SimpleLossCompute(model.generator, criterion,  model_opt))\n",
    "        model.eval()\n",
    "        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
    "                          model, \n",
    "                          SimpleLossCompute(model.generator, criterion,  None))\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(valid_iter):\n",
    "# for step, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(val_loader):\n",
    "    src = batch.src.transpose(0, 1)[:1]\n",
    "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
    "    out = greedy_decode(model, src, src_mask, \n",
    "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
    "    print(\"Translation:\", end=\"\\t\")\n",
    "    trans = \"<s> \"\n",
    "    for i in range(1, out.size(1)):\n",
    "        sym = TGT.vocab.itos[out[0, i]]\n",
    "        if sym == \"</s>\": break\n",
    "        trans += sym + \" \"\n",
    "        print(sym, end =\" \")\n",
    "    print()\n",
    "    print(\"Target:\", end=\"\\t\")\n",
    "    for i in range(1, batch.trg.size(0)):\n",
    "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
    "        if sym == \"</s>\": break\n",
    "        print(sym, end =\" \")\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tgt_sent = trans.split()\n",
    "def draw(data, x, y, ax):\n",
    "    seaborn.heatmap(data, \n",
    "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
    "                    cbar=False, ax=ax)\n",
    "    \n",
    "for layer in range(1, 6, 2):\n",
    "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "    print(\"Encoder Layer\", layer+1)\n",
    "    for h in range(4):\n",
    "        draw(model.encoder.layers[layer].self_attn.attn[0, h].data, \n",
    "            sent, sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()\n",
    "    \n",
    "for layer in range(1, 6, 2):\n",
    "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "    print(\"Decoder Self Layer\", layer+1)\n",
    "    for h in range(4):\n",
    "        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(tgt_sent)], \n",
    "            tgt_sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()\n",
    "    print(\"Decoder Src Layer\", layer+1)\n",
    "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
    "    for h in range(4):\n",
    "        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(sent)], \n",
    "            sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Decoder Self Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Decoder Src Layer 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
