{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        # hidden is 32 by 256\n",
    "        # encoder_outputs is 32 by 22 by 256\n",
    "        batch_size = hidden.size()[0]\n",
    "        attn_energies = []\n",
    "        for i in range(batch_size):\n",
    "            attn_energies.append(score(hidden[i], encoder_output[i]))\n",
    "        \n",
    "        # attn_energies is 32 by 22\n",
    "        attn_energies = self.softmax(torch.tensor(attn_energies))\n",
    "        \n",
    "        context_vectors = []\n",
    "        for i in range(batch_size):\n",
    "            context_vectors.append(torch.matmul(attn_energies[i], encoder_output[i]))\n",
    "            \n",
    "        context_vectors = torch.tensor(context_vectors)\n",
    "        \n",
    "        # Return context vectors\n",
    "        return context_vectors\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 22 by 256\n",
    "            encoder_output = torch.transpose(encoder_output, 0, 1)\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, self.attn(encoder_output))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            len_encoder_output = encoder_output.size()[1]\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            hidden = torch.transpose(hidden, 0, 1)\n",
    "            # hidden is 256 by 1\n",
    "            hidden = hidden.repeat(hidden_size, len_encoder_output)\n",
    "            # hidden is 256 by 22\n",
    "            concat = torch.cat((hidden, encoder_output), dim=0)\n",
    "            # concat is 512 by 22\n",
    "            # self.attn(concat) --> 256 by 22\n",
    "            energy = torch.matmul(self.v, F.tanh(self.attn(concat)))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context = self.attn(rnn_output, encoder_outputs)\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Batch_AttentionRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder_Batch_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, sents, sent_lengths):\n",
    "        '''\n",
    "            sents is a tensor with the shape (batch_size, padded_length )\n",
    "            when we evaluate sentence by sentence, you evaluate it with batch_size = 1, padded_length.\n",
    "            [[1, 2, 3, 4]] etc. \n",
    "        '''\n",
    "        batch_size = sents.size()[0]\n",
    "        sent_lengths = list(sent_lengths)\n",
    "        # We sort and then do pad packed sequence here. \n",
    "        descending_lengths = [x for x, _ in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_indices = [x for _, x in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_lengths = np.array(descending_lengths)\n",
    "        descending_sents = torch.index_select(sents, 0, torch.tensor(descending_indices).to(device))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(descending_sents)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, descending_lengths, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        packed_outputs, self.hidden = self.gru(embed, self.hidden)\n",
    "        # change the order back\n",
    "        change_it_back = [x for _, x in sorted(zip(descending_indices, range(len(descending_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device))\n",
    "        unpacked_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # reorder them\n",
    "        outputs = torch.index_select(rnn_out, 0, torch.LongTensor(change_it_back).to(device))\n",
    "        \n",
    "        return outputs, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "# example of input_tensor: [2, 43, 23, 9, 19, 4]. Indexed on our vocabulary. \n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # iterate GRU over words --> final hidden state is representation of source sentence. \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        # encoder_output and encoder_hidden are the same vectors\n",
    "        # 1 by 1 by 256\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0505, -1.3764, -0.7923]])\n",
      "tensor([[ 0.0505],\n",
      "        [-1.3764],\n",
      "        [-0.7923]])\n",
      "tensor([[ 0.0505,  0.0505],\n",
      "        [-1.3764, -1.3764],\n",
      "        [-0.7923, -0.7923]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(3, 1)\n",
    "print(torch.transpose(z, 0, 1))\n",
    "print(z)\n",
    "print(z.repeat(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Batch_AttentionRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder_Batch_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, word, decoder_hidden, encoder_outputs):\n",
    "        \n",
    "        # embed the word\n",
    "        embed = self.embedding(word)\n",
    "        # 1 by 256\n",
    "        \n",
    "        # calculate the attention weights\n",
    "        # decoder_hidden z = 1 by 256\n",
    "        # encoder_outputs is 22 by 256\n",
    "        # encoder_outputs = torch.transpose(encoder_outputs, 0, 1)\n",
    "        # encoder_outputs is 256 by 22\n",
    "        \n",
    "        # make z 256 by 1\n",
    "        # decoder_hidden = torch.transpose(decoder_hidden, 0, 1)\n",
    "        # now it's 256 by 1\n",
    "        # stack_zs = decoder_hidden.repeat(1, 22)\n",
    "        # now it's 256 by 22\n",
    "        # concatenated = torch.cat((stack_zs, encoder_outputs), dim=0)\n",
    "        # now it's 512 by 22\n",
    "        # linear layer (512, 1)\n",
    "        # now you have attn_numbers = 1 by 22\n",
    "        # softmax --> attn_weights\n",
    "        # context vector --> linear combination of attn_weights, encoder_outputs\n",
    "        # previous decoder hidden = decoder_hidden\n",
    "        \n",
    "        # concatenate decoder_hidden (z), context vector (c), and embed together\n",
    "        # gru (3*hidden_size, hidden_size)\n",
    "        # self.hidden is 256 by 1\n",
    "        # output = self.log_softmax(self.out(self.hidden), dim=1)\n",
    "        # return output\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hidden = \n",
    "\n",
    "        # get embedding\n",
    "        embed = self.embedding(descending_sents)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, descending_lengths, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = hidden\n",
    "        rnn_out, self.hidden = self.gru(embed, self.hidden)\n",
    "        \n",
    "        change_it_back = [x for _, x in sorted(zip(descending_indices, range(len(descending_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device))\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        output = self.softmax(self.out(rnn_out))\n",
    "        # now output is the size 28 by 31257 (vocab size)\n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every=1000, plot_every=1000, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang object for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    Max length generation is the max length generation you want \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = [] \n",
    "    count = 0 \n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    val_loss_total = 0\n",
    "    plot_val_loss = 0\n",
    "    encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate)\n",
    "    #encoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode=\"min\")\n",
    "    #decoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode=\"min\")\n",
    "\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token) # this ignores the padded token. \n",
    "    plot_loss =[]\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        plot_loss = []\n",
    "        val_loss = []\n",
    "        for step, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(train_loader):\n",
    "            encoder.train() # what is this for?\n",
    "            decoder.train()\n",
    "            \n",
    "            sent1_batch, sent2_batch = sent1s.to(device), sent2s.to(device) \n",
    "            sent1_length_batch, sent2_length_batch = sent1_lengths.to(device), sent2_lengths.to(device)\n",
    "            batch_size = sent1_batch.size()[0]\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            encoder_outputs, encoder_hidden = encoder(sent1_batch, sent1_length_batch)\n",
    "            # encoder_outputs is 32 by 22 by 256\n",
    "            # encoder_hidden is 1 by 32 by 256\n",
    "            \n",
    "            decoder_hidden = encoder_hidden     \n",
    "            \n",
    "            # decoder_input is incoming word token\n",
    "            # decoder_hidden starts the last hidden state of the encoder\n",
    "            # encoder_outputs is all of the encoder hidden states\n",
    "            for i in range(batch_size):\n",
    "                # we're going over each sentence in the batch\n",
    "                decoder_hidden = encoder_hidden[i] # 1 by 256\n",
    "                encoder_outputs = encoder_outputs[i] # 1 by 22 by 256\n",
    "                decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "                for j in range(len(sent2_batch[i])):\n",
    "                    # we're going over each word in the target sentence\n",
    "                    # \"I am a robot\"\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    loss += criterion(decoder_output, target_tensor[di])\n",
    "                    decoder_input = sent2_batch[i][j]# Teacher forcing\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
