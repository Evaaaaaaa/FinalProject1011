{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sacrebleu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8e96ab9e8b04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_architectures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder_RNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder_RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_prep\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorsFromPair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepareNonTrainDataForLanguagePair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_cpickle_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minference\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimeSince\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_cpickle_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/FinalProject1011/inference.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msacrebleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sacrebleu'"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "from model_architectures import Encoder_RNN, Decoder_RNN\n",
    "from data_prep import prepareData, tensorsFromPair, prepareNonTrainDataForLanguagePair, load_cpickle_gc\n",
    "from inference import generate_translation\n",
    "from misc import timeSince, load_cpickle_gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "teacher_forcing_ratio = 1.0\n",
    "attn_model = 'dot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_lang, target_lang, train_pairs = prepareData(\n",
    "#     \"iwslt-vi-en-processed/train.vi\",\n",
    "#     \"iwslt-vi-en-processed/train.en\",\n",
    "#     input_lang = 'vi',\n",
    "#     target_lang = 'en')\n",
    "\n",
    "# train_idx_pairs = []\n",
    "# for x in train_pairs:\n",
    "#     indexed = list(tensorsFromPair(x, input_lang, target_lang))\n",
    "#     train_idx_pairs.append(indexed)\n",
    "\n",
    "# pickle.dump(input_lang, open(\"input_lang_vi\", \"wb\"))\n",
    "# pickle.dump(target_lang, open(\"target_lang_en\", \"wb\"))\n",
    "# pickle.dump(train_idx_pairs, open(\"train_vi_en_idx_pairs\", \"wb\"))\n",
    "\n",
    "# _, _, val_pairs = prepareData(\n",
    "#     \"iwslt-vi-en-processed/dev.vi\",\n",
    "#     \"iwslt-vi-en-processed/dev.en\",\n",
    "#     input_lang = 'vi',\n",
    "#     target_lang = 'en')\n",
    "\n",
    "# val_idx_pairs = []\n",
    "# for x in val_pairs:\n",
    "#     indexed = list(tensorsFromPair(x, input_lang, target_lang))\n",
    "#     val_idx_pairs.append(indexed)\n",
    "\n",
    "# pickle.dump(val_pairs, open(\"val_pairs\", \"wb\"))\n",
    "# pickle.dump(val_idx_pairs, open(\"val_idx_pairs\", \"wb\"))\n",
    "\n",
    "\n",
    "input_lang = load_cpickle_gc(\"input_lang_vi\")\n",
    "target_lang = load_cpickle_gc(\"target_lang_en\")\n",
    "train_idx_pairs = load_cpickle_gc(\"train_vi_en_idx_pairs\")\n",
    "val_idx_pairs = load_cpickle_gc(\"val_idx_pairs\")\n",
    "val_pairs = load_cpickle_gc(\"val_pairs\")\n",
    "\n",
    "train_dataset = LanguagePairDataset(train_idx_pairs)\n",
    "# is there anything in the train_idx_pairs that is only 0s right noww instea dof padding. \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=language_pair_dataset_collate_function,\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check SOS Token: Do both train and val start with that? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Batch_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder_Batch_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def forward(self, sents, sent_lengths):\n",
    "        '''\n",
    "            sents is (batch_size by padded_length)\n",
    "            when we evaluate sentence by sentence, you evaluate it with batch_size = 1, padded_length.\n",
    "            [[1, 2, 3, 4]] etc. \n",
    "        '''\n",
    "        batch_size = sents.size()[0]\n",
    "        sent_lengths = list(sent_lengths)\n",
    "        # We sort and then do pad packed sequence here. \n",
    "        descending_lengths = [x for x, _ in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_indices = [x for _, x in sorted(zip(sent_lengths, range(len(sent_lengths))), reverse=True)]\n",
    "        descending_lengths = np.array(descending_lengths)\n",
    "        descending_sents = torch.index_select(sents, 0, torch.tensor(descending_indices).to(device))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(descending_sents)\n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, descending_lengths, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        rnn_out, self.hidden = self.gru(embed, self.hidden)\n",
    "        rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        \n",
    "        # change the order back\n",
    "        change_it_back = [x for _, x in sorted(zip(descending_indices, range(len(descending_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device))  \n",
    "        print(\"Setting trace in encoder_batch...verify order of rnn_out and hidden\")\n",
    "        pdb.set_trace()\n",
    "        rnn_out = torch.index_select(rnn_out, 1, torch.LongTensor(change_it_back).to(device)) \n",
    "        \n",
    "        return rnn_out, self.hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        # hidden is 32 by 256\n",
    "        # encoder_outputs is 32 by 22 by 256\n",
    "        batch_size = hidden.size()[0]\n",
    "        attn_energies = []\n",
    "        for i in range(batch_size):\n",
    "            attn_energies.append(score(hidden[i], encoder_output[i]))\n",
    "        \n",
    "        # attn_energies is 32 by 22\n",
    "        attn_energies = self.softmax(torch.tensor(attn_energies))\n",
    "        \n",
    "        context_vectors = []\n",
    "        for i in range(batch_size):\n",
    "            context_vectors.append(torch.matmul(attn_energies[i], encoder_output[i]))\n",
    "            \n",
    "        context_vectors = torch.tensor(context_vectors)\n",
    "        \n",
    "        # Return context vectors\n",
    "        return context_vectors\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':            \n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 22 by 256\n",
    "            encoder_output = torch.transpose(encoder_output, 0, 1)\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            energy = torch.matmul(hidden, self.attn(encoder_output))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            len_encoder_output = encoder_output.size()[1]\n",
    "            # hidden is 1 by 256\n",
    "            # encoder_output is 256 by 22\n",
    "            hidden = torch.transpose(hidden, 0, 1)\n",
    "            # hidden is 256 by 1\n",
    "            hidden = hidden.repeat(hidden_size, len_encoder_output)\n",
    "            # hidden is 256 by 22\n",
    "            concat = torch.cat((hidden, encoder_output), dim=0)\n",
    "            # concat is 512 by 22\n",
    "            # self.attn(concat) --> 256 by 22\n",
    "            energy = torch.matmul(self.v, F.tanh(self.attn(concat)))\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        print(\"Setting trace in forward of LuongAttentionDecoder\")\n",
    "        pdb.set_trace()\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        context = self.attn(rnn_output, encoder_outputs)\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, pairs, validation_pairs, lang1, lang2, search, title, max_length_generation,  print_every=1000, plot_every=1000, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang object for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    Max length generation is the max length generation you want \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    plot_losses, val_losses = []\n",
    "    val_losses = [] \n",
    "    count, print_loss_total, plot_loss_total, val_loss_total, plot_val_loss = 0, 0, 0, 0, 0 \n",
    "    encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate)\n",
    "    #encoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode=\"min\")\n",
    "    #decoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode=\"min\")\n",
    "\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token) # this ignores the padded token. \n",
    "    plot_loss =[]\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        plot_loss = []\n",
    "        val_loss = []\n",
    "        for step, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(train_loader):\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            sent1_batch, sent2_batch = sent1s.to(device), sent2s.to(device) \n",
    "            sent1_length_batch, sent2_length_batch = sent1_lengths.to(device), sent2_lengths.to(device)\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            outputs, encoder_hidden = encoder(sent1_batch, sent1_length_batch)\n",
    "            \n",
    "            decoder_input = torch.LongTensor([START_TOKEN] * batch_size).to(device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            \n",
    "            max_trg_len = max(sent2_lengths)\n",
    "            \n",
    "            # Run through decoder one time step at a time using TEACHER FORCING=1.0\n",
    "            for t in range(max_trg_len):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs\n",
    "                )\n",
    "                pdb.set_trace()\n",
    "                loss += criterion(decoder_output, sent2_batch[t])\n",
    "                decoder_input = sent2_batch[:, t]\n",
    " \n",
    "\n",
    "            loss = loss / max_trg_len\n",
    "            loss.backward()\n",
    "        \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            if (step+1) % print_every == 0:\n",
    "                # lets train and plot at the same time. \n",
    "                print_loss_avg = print_loss_total / count\n",
    "                count = 0\n",
    "                print_loss_total = 0\n",
    "                print('TRAIN SCORE %s (%d %d%%) %.4f' % (timeSince(start, step / n_epochs),\n",
    "                                             step, step / n_epochs * 100, print_loss_avg))\n",
    "                # 42s\n",
    "#                 v_loss = test_model(encoder, decoder, search, validation_pairs, lang2, max_length=max_length_generation)\n",
    "                # returns bleu score\n",
    "#                 print(\"VALIDATION BLEU SCORE: \"+str(v_loss))\n",
    "#                 val_loss.append(v_loss)\n",
    "                plot_loss.append(print_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    save_model(encoder, decoder, val_losses, plot_losses, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = Encoder_Batch_RNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = Decoder_Batch_2RNN(target_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = LuongAttnDecoderRNN(attn_model, hidden_size, target_lang.n_words, 1).to(device)\n",
    "args = {\n",
    "    'n_epochs': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'search': 'beam',\n",
    "    'encoder': encoder1,\n",
    "    'decoder': decoder1,\n",
    "    'lang1': input_lang, \n",
    "    'lang2': target_lang,\n",
    "    \"pairs\":train_idx_pairs, \n",
    "    \"validation_pairs\": val_idx_pairs[:200], \n",
    "    \"title\": \"Training Curve for Basic 1-Directional Encoder Decoder Model With LR = 0.0001\",\n",
    "    \"max_length_generation\": 20, \n",
    "    \"plot_every\": 100, \n",
    "    \"print_every\": 100\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "We follow https://arxiv.org/pdf/1406.1078.pdf \n",
    "and use the Adadelta optimizer\n",
    "\n",
    "\"\"\"\n",
    "print(BATCH_SIZE)\n",
    "\n",
    "trainIters(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every=1000, plot_every=1000, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    lang1 is the Lang object for language 1 \n",
    "    Lang2 is the Lang object for language 2\n",
    "    Max length generation is the max length generation you want \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = [] \n",
    "    count = 0 \n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    val_loss_total = 0\n",
    "    plot_val_loss = 0\n",
    "    encoder_optimizer = torch.optim.Adadelta(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adadelta(decoder.parameters(), lr=learning_rate)\n",
    "    #encoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, mode=\"min\")\n",
    "    #decoder_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, mode=\"min\")\n",
    "\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token) # this ignores the padded token. \n",
    "    plot_loss =[]\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        plot_loss = []\n",
    "        val_loss = []\n",
    "        for step, (sent1s, sent1_lengths, sent2s, sent2_lengths) in enumerate(train_loader):\n",
    "            encoder.train() # what is this for?\n",
    "            decoder.train()\n",
    "            \n",
    "            sent1_batch, sent2_batch = sent1s.to(device), sent2s.to(device) \n",
    "            sent1_length_batch, sent2_length_batch = sent1_lengths.to(device), sent2_lengths.to(device)\n",
    "            batch_size = sent1_batch.size()[0]\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            encoder_outputs, encoder_hidden = encoder(sent1_batch, sent1_length_batch)\n",
    "            # encoder_outputs is 32 by 22 by 256\n",
    "            # encoder_hidden is 1 by 32 by 256\n",
    "            \n",
    "            decoder_hidden = encoder_hidden     \n",
    "            \n",
    "            # decoder_input is incoming word token\n",
    "            # decoder_hidden starts the last hidden state of the encoder\n",
    "            # encoder_outputs is all of the encoder hidden states\n",
    "            for i in range(batch_size):\n",
    "                # we're going over each sentence in the batch\n",
    "                decoder_hidden = encoder_hidden[i] # 1 by 256\n",
    "                encoder_outputs = encoder_outputs[i] # 1 by 22 by 256\n",
    "                decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "                for j in range(len(sent2_batch[i])):\n",
    "                    # we're going over each word in the target sentence\n",
    "                    # \"I am a robot\"\n",
    "                    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                        decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    loss += criterion(decoder_output, target_tensor[di])\n",
    "                    decoder_input = sent2_batch[i][j]# Teacher forcing\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
